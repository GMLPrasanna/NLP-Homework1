{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsA5uo9-qyR8",
        "outputId": "be713c31-a6e3-43d4-8e4e-7998df6ef244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: top pair = ('n', 'e'), vocab size = 12\n",
            "Step 2: top pair = ('ne', 'w'), vocab size = 13\n",
            "Step 3: top pair = ('e', 's'), vocab size = 14\n",
            "Step 4: top pair = ('es', 't'), vocab size = 15\n",
            "Step 5: top pair = ('est', '_'), vocab size = 16\n",
            "Step 6: top pair = ('new', '_'), vocab size = 17\n",
            "Step 7: top pair = ('new', 'e'), vocab size = 18\n",
            "Step 01: top pair = ('s', '_')  |  count =  12  |  vocab size = 26\n",
            "Step 02: top pair = ('a', 'n')  |  count =   8  |  vocab size = 27\n",
            "Step 03: top pair = ('e', '_')  |  count =   7  |  vocab size = 28\n",
            "Step 04: top pair = ('g', 'e_')  |  count =   5  |  vocab size = 29\n",
            "Step 05: top pair = ('i', 'n')  |  count =   5  |  vocab size = 30\n",
            "Step 06: top pair = ('e', 'r')  |  count =   5  |  vocab size = 31\n",
            "Step 07: top pair = ('l', 'an')  |  count =   4  |  vocab size = 32\n",
            "Step 08: top pair = ('lan', 'g')  |  count =   4  |  vocab size = 33\n",
            "Step 09: top pair = ('lang', 'u')  |  count =   4  |  vocab size = 34\n",
            "Step 10: top pair = ('langu', 'a')  |  count =   4  |  vocab size = 35\n",
            "Step 11: top pair = ('langua', 'ge_')  |  count =   4  |  vocab size = 36\n",
            "Step 12: top pair = ('r', 'o')  |  count =   4  |  vocab size = 37\n",
            "Step 13: top pair = ('a', 't')  |  count =   3  |  vocab size = 38\n",
            "Step 14: top pair = ('p', 'ro')  |  count =   3  |  vocab size = 39\n",
            "Step 15: top pair = ('in', 'g')  |  count =   3  |  vocab size = 40\n",
            "Step 16: top pair = ('c', 'o')  |  count =   3  |  vocab size = 41\n",
            "Step 17: top pair = ('u', 'n')  |  count =   3  |  vocab size = 42\n",
            "Step 18: top pair = ('e', 'a')  |  count =   3  |  vocab size = 43\n",
            "Step 19: top pair = ('n', '_')  |  count =   3  |  vocab size = 44\n",
            "Step 20: top pair = ('w', 'o')  |  count =   3  |  vocab size = 45\n",
            "Step 21: top pair = ('wo', 'r')  |  count =   3  |  vocab size = 46\n",
            "Step 22: top pair = ('wor', 'd')  |  count =   3  |  vocab size = 47\n",
            "Step 23: top pair = ('a', 'l')  |  count =   2  |  vocab size = 48\n",
            "Step 24: top pair = ('e', 's')  |  count =   2  |  vocab size = 49\n",
            "Step 25: top pair = ('ing', '_')  |  count =   2  |  vocab size = 50\n",
            "Step 26: top pair = ('e', 'l')  |  count =   2  |  vocab size = 51\n",
            "Step 27: top pair = ('t', 'er')  |  count =   2  |  vocab size = 52\n",
            "Step 28: top pair = ('an', 'd')  |  count =   2  |  vocab size = 53\n",
            "Step 29: top pair = ('o', 'd')  |  count =   2  |  vocab size = 54\n",
            "Step 30: top pair = ('ea', 'r')  |  count =   2  |  vocab size = 55\n",
            "\n",
            "--- Five most frequent merges (by frequency at time learned) ---\n",
            "('s', '_')  ->  s_   (count=12)\n",
            "('a', 'n')  ->  an   (count=8)\n",
            "('e', '_')  ->  e_   (count=7)\n",
            "('g', 'e_')  ->  ge_   (count=5)\n",
            "('i', 'n')  ->  in   (count=5)\n",
            "\n",
            "--- Five longest subword tokens in final vocabulary ---\n",
            "language_\n",
            "langua\n",
            "langu\n",
            "word\n",
            "lang\n",
            "\n",
            "--- Segmentations (tokens include end-of-word _ where applicable) ---\n",
            "tokenization   -> t o k e n i z at i o n_\n",
            "collections    -> co l l e c t i o n s_\n",
            "processing     -> pro c es s ing_\n",
            "unknown        -> un k n o w n_\n",
            "technologies   -> t e c h n o l o g i e s_\n",
            "['నేను', 'ఈ', 'సినిమా', 'చాలా', 'బాగుంది!', 'ఇది', 'ఈ', 'సంవత్సరం', 'చూసిన', 'ఉత్తమ', 'చిత్రాల్లో', 'ఒకటి.', 'చూడండి,', 'మీరు', 'ఖచ్చితంగా', 'ఇష్టపడతారు.']\n",
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.12/dist-packages (0.92)\n",
            "Requirement already satisfied: sphinx-argparse in /usr/local/lib/python3.12/dist-packages (from indic-nlp-library) (0.5.2)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.12/dist-packages (from indic-nlp-library) (3.1.0)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.12/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from indic-nlp-library) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from indic-nlp-library) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->indic-nlp-library) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->indic-nlp-library) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->indic-nlp-library) (2025.3)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from sphinx-argparse->indic-nlp-library) (8.2.3)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.12/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.12/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.19.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.1)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.18.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.4)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (4.1.0)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (26.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2026.1.4)\n",
            "Requirement already satisfied: roman-numerals==4.1.0 in /usr/local/lib/python3.12/dist-packages (from roman-numerals-py>=1.0.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (4.1.0)\n",
            "['నేను', 'ఈ', 'సినిమా', 'చాలా', 'బాగుంది', '!', 'ఇది', 'ఈ', 'సంవత్సరం', 'చూసిన', 'ఉత్తమ', 'చిత్రాల్లో', 'ఒకటి', '.', 'చూడండి', ',', 'మీరు', 'ఖచ్చితంగా', 'ఇష్టపడతారు', '.']\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"NLP HW1 .ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1T4PnsiYWNGlhunO0vTqcDOFYoZGc8153\n",
        "\"\"\"\n",
        "\n",
        "# question 2.2 task 1.\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "corpus = [\"new\", \"newer\", \"lowest\", \"widest\"]\n",
        "\n",
        "# initialize with characters + end-of-word\n",
        "def init_vocab(words):\n",
        "    return {tuple(list(w) + [\"_\"]): 1 for w in words}\n",
        "\n",
        "def get_pair_counts(vocab):\n",
        "    counts = Counter()\n",
        "    for tokens, freq in vocab.items():\n",
        "        for i in range(len(tokens)-1):\n",
        "            counts[(tokens[i], tokens[i+1])] += freq\n",
        "    return counts\n",
        "\n",
        "def merge_pair(pair, vocab):\n",
        "    new_vocab = {}\n",
        "    a, b = pair\n",
        "    for tokens, freq in vocab.items():\n",
        "        new = []\n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            if i < len(tokens)-1 and tokens[i] == a and tokens[i+1] == b:\n",
        "                new.append(a+b)\n",
        "                i += 2\n",
        "            else:\n",
        "                new.append(tokens[i])\n",
        "                i += 1\n",
        "        new_vocab[tuple(new)] = freq\n",
        "    return new_vocab\n",
        "\n",
        "vocab = init_vocab(corpus)\n",
        "symbols = set(ch for w in corpus for ch in w) | {\"_\"}\n",
        "\n",
        "for step in range(1, 8):\n",
        "    pair_counts = get_pair_counts(vocab)\n",
        "    if not pair_counts: break\n",
        "    top_pair = pair_counts.most_common(1)[0][0]\n",
        "    vocab = merge_pair(top_pair, vocab)\n",
        "    symbols.add(\"\".join(top_pair))\n",
        "    print(f\"Step {step}: top pair = {top_pair}, vocab size = {len(symbols)}\")\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# -----------------------------\n",
        "# 2.3 — BPE on a short paragraph\n",
        "# -----------------------------\n",
        "\n",
        "PARAGRAPH = (\n",
        "    \"Natural language processing helps computers understand human language. \"\n",
        "    \"Language models learn patterns from large text collections. \"\n",
        "    \"Tokenization breaks words into smaller meaningful units. \"\n",
        "    \"Subword methods improve handling of unknown words. \"\n",
        "    \"Researchers continuously improve language technologies.\"\n",
        ")\n",
        "\n",
        "EOW = \"_\"          # end-of-word marker (assignment requirement)\n",
        "NUM_MERGES = 30    # learn at least 30 merges\n",
        "\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def words_from_paragraph(text: str):\n",
        "    # Basic word extraction: keep letters only, lowercase\n",
        "    cleaned = []\n",
        "    cur = []\n",
        "    for ch in text.lower():\n",
        "        if ch.isalpha():\n",
        "            cur.append(ch)\n",
        "        else:\n",
        "            if cur:\n",
        "                cleaned.append(\"\".join(cur))\n",
        "                cur = []\n",
        "    if cur:\n",
        "        cleaned.append(\"\".join(cur))\n",
        "    return cleaned\n",
        "\n",
        "def init_corpus(words):\n",
        "    # Represent each word as a list of characters + end-of-word marker\n",
        "    return [list(w) + [EOW] for w in words]\n",
        "\n",
        "def get_pair_counts(corpus):\n",
        "    counts = Counter()\n",
        "    for tokens in corpus:\n",
        "        for i in range(len(tokens) - 1):\n",
        "            counts[(tokens[i], tokens[i+1])] += 1\n",
        "    return counts\n",
        "\n",
        "def merge_pair_in_tokens(tokens, pair):\n",
        "    a, b = pair\n",
        "    merged = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        if i < len(tokens) - 1 and tokens[i] == a and tokens[i+1] == b:\n",
        "            merged.append(a + b)\n",
        "            i += 2\n",
        "        else:\n",
        "            merged.append(tokens[i])\n",
        "            i += 1\n",
        "    return merged\n",
        "\n",
        "def apply_merge_to_corpus(corpus, pair):\n",
        "    return [merge_pair_in_tokens(tokens, pair) for tokens in corpus]\n",
        "\n",
        "def learn_bpe(words, num_merges=30):\n",
        "    corpus = init_corpus(words)\n",
        "    merges = []              # list of pairs in the order learned\n",
        "    merge_freqs = []         # frequency of the chosen pair at that step\n",
        "\n",
        "    # initial vocabulary = all unique symbols (characters + EOW)\n",
        "    vocab = set(sym for w in corpus for sym in w)\n",
        "\n",
        "    for step in range(1, num_merges + 1):\n",
        "        pair_counts = get_pair_counts(corpus)\n",
        "        if not pair_counts:\n",
        "            break\n",
        "\n",
        "        top_pair, top_count = pair_counts.most_common(1)[0]\n",
        "\n",
        "        # record\n",
        "        merges.append(top_pair)\n",
        "        merge_freqs.append((top_pair, top_count))\n",
        "\n",
        "        # merge everywhere in corpus\n",
        "        corpus = apply_merge_to_corpus(corpus, top_pair)\n",
        "\n",
        "        # add merged symbol to vocabulary\n",
        "        vocab.add(top_pair[0] + top_pair[1])\n",
        "\n",
        "        print(f\"Step {step:02d}: top pair = {top_pair}  |  count = {top_count:3d}  |  vocab size = {len(vocab)}\")\n",
        "\n",
        "    return merges, merge_freqs, vocab\n",
        "\n",
        "def bpe_segment_word(word, merges):\n",
        "    # segmenter: apply merges greedily, in learned order\n",
        "    tokens = list(word.lower()) + [EOW]\n",
        "    for pair in merges:\n",
        "        tokens = merge_pair_in_tokens(tokens, pair)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# ---------- Run BPE ----------\n",
        "words = words_from_paragraph(PARAGRAPH)\n",
        "merges, merge_freqs, vocab = learn_bpe(words, NUM_MERGES)\n",
        "\n",
        "print(\"\\n--- Five most frequent merges (by frequency at time learned) ---\")\n",
        "top5_merges = sorted(merge_freqs, key=lambda x: x[1], reverse=True)[:5]\n",
        "for pair, cnt in top5_merges:\n",
        "    print(f\"{pair}  ->  {pair[0] + pair[1]}   (count={cnt})\")\n",
        "\n",
        "print(\"\\n--- Five longest subword tokens in final vocabulary ---\")\n",
        "# longest tokens by string length (excluding single chars if you want, but we keep all)\n",
        "longest5 = sorted(vocab, key=lambda s: (len(s), s), reverse=True)[:5]\n",
        "for tok in longest5:\n",
        "    print(tok)\n",
        "\n",
        "# ---------- Segment 5 words from the paragraph ----------\n",
        "# Pick 5 words present in the paragraph; include one rare and one derived/inflected form.\n",
        "# (Example choices: \"tokenization\" (derived), \"collections\" (rarer), plus a few others)\n",
        "targets = [\"tokenization\", \"collections\", \"processing\", \"unknown\", \"technologies\"]\n",
        "\n",
        "print(\"\\n--- Segmentations (tokens include end-of-word _ where applicable) ---\")\n",
        "for w in targets:\n",
        "    seg = bpe_segment_word(w, merges)\n",
        "    print(f\"{w:14s} -> {' '.join(seg)}\")\n",
        "\n",
        "#Question 5. 1.\n",
        "# Naive space-based tokenization (Telugu or any language)\n",
        "# Splits ONLY on whitespace, so punctuation stays attached.\n",
        "\n",
        "text = \"నేను ఈ సినిమా చాలా బాగుంది! ఇది ఈ సంవత్సరం చూసిన ఉత్తమ చిత్రాల్లో ఒకటి. చూడండి, మీరు ఖచ్చితంగా ఇష్టపడతారు.\"\n",
        "\n",
        "tokens = text.split()   # naive whitespace split\n",
        "print(tokens)\n",
        "\n",
        "!pip install indic-nlp-library\n",
        "\n",
        "#question 5.2. Tool comparision\n",
        "\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "text = \"నేను ఈ సినిమా చాలా బాగుంది! ఇది ఈ సంవత్సరం చూసిన ఉత్తమ చిత్రాల్లో ఒకటి. చూడండి, మీరు ఖచ్చితంగా ఇష్టపడతారు.\"\n",
        "\n",
        "tool_tokens = indic_tokenize.trivial_tokenize(text, lang=\"te\")\n",
        "print(tool_tokens)"
      ]
    }
  ]
}